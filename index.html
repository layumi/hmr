<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(10, 10, 10, 0),
    rgba(10, 10, 10, 0.75), rgba(10, 10, 10, 0));
        margin: 1em 0 1em 0;
    }
</style>

<html>
  <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-10550309-6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-10550309-6');
</script>
        <title>Human Mesh Recovery</title>
        <meta property="og:title" content="HMR" />	
  </head>

  <body>
    <br>
    <center>
      <span style="font-size:42px">End-to-end Recovery of Human Shape and Pose
	</span>
    </center>

    <br><br>
      <table align=center width=900px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="http://www.cs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a></span>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://ps.is.tuebingen.mpg.de/person/black">Michael
        J Black</a></span>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://www.cs.umd.edu/~djacobs/">David
        W. Jacobs</a></span>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="http://www.eecs.berkeley.edu/~malik/">Jitendra Malik</a></span>
        </center>
        </td>
     </tr>
    </table>

    <br>
    <table align=center width=700px>
       <tr>
        <td align=center width=100px>
        <center>
          <span style="font-size:20px">University of California, Berkeley</span><br>
<span style="font-size:20px">MPI for Intelligent Systems, T&uumlbingen, Germany<br> University of Maryland, College Park
</span></center>
        </td>
     </tr>
    </table>

            <br>
            <table align=center width=900px>
                <tr>
                    <td width=600px>
                      <center>
                          <a href="./resources/images/teaser.png"><img src = "./resources/images/teaser.png" height="400px"></img></href></a><br>
                    </center>
                    </td>
                </tr>
                    <td width=600px>
                      <!-- <center> -->
                          <span style="font-size:14px"><i> <span style="font-weight:bold">Human
                          Mesh Recovery (HMR): End-to-end adversarial learning of human pose and shape.</span> We present a real time framework for recovering the 3D joint angles and shape of the body from a single RGB image. Bottom row shows results from a model trained without using any coupled 2D-to-3D supervision. We infer the full 3D body even in case of occlusions and truncations. Note that we capture head and limb orientations.</i>
                    <!-- </center> -->
                    </td>
                </tr>
            </table>

            <br>
	    We present Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full
	    3D mesh of a human body from a single RGB image. 
	    In contrast to most current methods that compute 2D or 3D joint
	    locations, we produce a richer and more useful mesh representation that is
	    parameterized by shape and 3D joint angles. The main objective is to minimize
	    the reprojection loss of keypoints, which allow our model to be trained using \emph{in-the-wild} images that only have
	    ground truth 2D annotations.
	    However, reprojection loss alone is highly under constrained.
	    In this work we address this problem by introducing an adversary trained to
	    tell whether a human body parameter is real or not using a large database of
	    3D human meshes. We show that HMR can be trained with and <b>without</b> using
	    any paired 2D-to-3D supervision.  We do not rely on intermediate 2D
	    keypoint detection and infer 3D pose and shape parameters directly
	    from image pixels. Our model runs in real-time given a bounding box
	    containing the person.  We demonstrate our approach on various images <i>in-the-wild</i> and out-perform previous optimization-based
	    methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.
            <br><br>
            <hr>
            <table align=center width=650>
              <center><h1>Paper</h1></center>
              <tr>
                <td style="padding:1em"><a href="https://arxiv.org/abs/1712.06584"><img style="height:180px" src="./resources/images/paper_thumb.png"/></a></td>
                <td style="padding:1em"><span style="font-size:14pt">Angjoo Kanazawa, Michael
                    J. Black, David W. Jacobs, Jitendra Malik.<br><br>
                    End-to-end Recovery of Human Shape and Pose<br><br>
                    arXiv, Dec 2017.<br> </span>
                </td>
              </tr>
            </table>
            <br>

            <table align=center width=180px>
              <tr>
                <td><span style="font-size:14pt"><center>
                      <a href="https://arxiv.org/pdf/1712.06584.pdf">[pdf]</a>
                </center></td>
		
                <td><span style="font-size:14pt"><center>
                      <a href="./resources/bibtex.txt">[Bibtex]</a>
                </center></td>
              </tr>
            </table>
            <br>
            <hr>
            <center><h1>Video</h1></center> 
            <table align=center width=1000px> 
              <tr> 
                <center> 
		  <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/bmMV9aJKa-c" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe> -->
		  <iframe width="840" height="472" src="https://www.youtube.com/embed/bmMV9aJKa-c" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
                </center> 
              </tr> 
            </table>	       
	    <br>	  
	    <hr>
            <center><h1>Overview</h1></center> 
            <table align=center width=1000px> 
              <tr> 
		<center> 
		  <img class="round" style="height:300" src="./resources/images/overview.png"/> 
		</center>
              <td width=600px>
		<!-- <center> -->
		<span style="font-size:14px"><i> <span style="font-weight:bold">Overview
		      of the proposed framework.</span> An image is passed
		    through a convolutional encoder and then to an
		    iterative 3D regression module that infers the
		    latent 3D representation of the human that
		    minimizes the joint reprojection error.
		    The 3D parameters are also sent to the discriminator D, whose goal is to tell if the 3D human is from a real data or not.</i></span>
		<!-- </center> -->
              </td>
              </tr>
	      <tr>
	      <td><center> <br> 
		  <span style="font-size:20px">&nbsp;<a href='https://github.com/akanazawa/hmr'>
		      Code [coming soon]</a> </span>
		  <br> 
		</center>
	      </td>
	      </tr>
	    </table>
	    <br>
		We present an end-to-end framework for recovering a full 3D mesh
		of a human body from a single RGB image. We use the generative
		human body model <a href="http://smpl.is.tue.mpg.de/">SMPL</a>,
		which parameterizes the mesh by 3D joint angles and a
		low-dimensional linear shape space. <!-- We estimate the 3D joint angles, -->
		<!-- the shape, as well as the weak-perspective camera of the input -->
		<!-- image. -->
		Estimating a 3D mesh opens the door to a wide range of applications such as foreground and
		part segmentation and dense correspondences that are beyond
		what is practical with a simple skeleton. The output mesh can be
		immediately used by animators, modified, measured, manipulated
		and retargeted. Our output is also holistic â€“ we always infer
		the full 3D body even in case of occlusions and
	    truncations. <br>
	    <br>
	    There are several challenges in training such an model in an end-to-end
	    manner:
	    <ol>
	      <li> First is the lack of large-scale ground truth 3D
		annotation for <i>in-the-wild</i> images. Existing datasets with
		accurate 3D annotations are captured in constrained
		environments
		(<a href="http://humaneva.is.tue.mpg.de/">HumanEva</a>
		, <a href="http://vision.imar.ro/human3.6m/description.php">Human3.6M</a>
		, <a href="http://gvv.mpi-inf.mpg.de/3dhp-dataset/">MPI-INF-3DHP</a>
		). Models trained on these datasets do not generalize
		well to the richness of images in the real world.
		
		<li> Second is the inherent ambiguities in single-view 2D-to-3D
		mapping. Many of these configurations may not be
		anthropometrically reasonable, such as impossible joint angles
		or extremely skinny bodies. In addition, estimating the camera explicitly introduces an additional scale ambiguity between the size of the person and the camera distance.
	    </ol>
	    In this work we propose a novel approach to mesh reconstruction that
	    addresses both of these challenges. The key insight is even though
	    we don't have a large-scale paired 2D-to-3D labels of images in-the-wild, we have
	    a lot of <i>unpaired</i> datasets: large-scale 2D keypoint
	    annotations of in-the-wild images
	    (<a href="http://sam.johnson.io/research/lsp.html">LSP</a>
	    , <a href="http://human-pose.mpi-inf.mpg.de/">MPII</a>
	    , <a href="http://cocodataset.org/#keypoints-challenge2017">COCO</a>
	    , etc) and a
	    separate large-scale dataset of 3D meshes of people with various
	    poses and shapes from MoCap. Our key contribution is to take
	    advantage of these <i>unpaired</i> 2D keypoint annotations and 3D
	    scans in a conditional generative adversarial manner. <br>

	    The idea is that, given an image, the network has to infer the 3D
	    mesh parameters and the camera such that the 3D keypoints match the
	    annotated 2D keypoints after projection. To deal with ambiguities,
	    these parameters are sent to a discriminator network, whose task is
	    to determine if the 3D parameters correspond to bodies of real
	    humans or not. Hence the network is encouraged to output parameters
	    on the human manifold and the discriminator acts as a weak
	    supervision. The network implicitly learns the angle limits for each
	    joint and is discouraged from making people with unusual body
	    shapes.
	    <br>
	    <br>
	    We take advantage of the structure of the body model and propose a
	    factorized adversarial prior. We show that we can train a model even
	    <i>without</i> using any paired 2D-to-3D training data (pink meshes are all
	    results of this unpaired model). Even without using any paired
	    2D-to-3D supervision, HMR produces reasonable 3D
	    reconstructions. This is most exciting because it opens up
	    possibilities for learning 3D from large amounts of 2D data.
	    <br>
	    <br>
	    Please see the <a href="https://arxiv.org/pdf/1712.06584.pdf">paper</a> for more details.
	    <hr>
	 <br>
            <table align=center width=1100px>
                <tr>
                    <td>
                      <left>
			<center><h1>Acknowledgements</h1></center>
			We thank Naureen Mahmood for providing MoShed
			datasets and  mesh retargeting for character animation, Dushyant
			Mehta for his assistance on MPI-INF-3DHP, and Shubham Tulsiani,
			Abhishek Kar, Saurabh Gupta, David Fouhey and Ziwei Liu for helpful
			discussions. This research was supported in part by <a href="http://bair.berkeley.edu/">BAIR</a> and NSF Award IIS-1526234.			
		This webpage template is taken
			from <a href="https://shubhtuls.github.io/drc/">humans
			working on 3D</a> who borrowed it
		from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>.
            </left>
        </td>
        </tr>
        </table>

        <br><br>
</body>
</html>
